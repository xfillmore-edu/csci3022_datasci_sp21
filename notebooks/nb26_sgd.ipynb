{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 26: Stochastic Gradient Descent\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need Numpy, Matplotlib, Pandas, and scipy.stats for this notebook, so let's load them.\n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and evaluate those before proceeding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pylab as plt \n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Stochastic Gradient Descent for Simple Linear Regression \n",
    "***\n",
    "\n",
    "In this exercise we will implement Stochastic Gradient Descent to learn the parameters of a simple linear regression.  We'll use data randomly generated from the model $Y = 1 + 2X + \\epsilon$ where $\\epsilon \\sim N(0,1)$. The following code will generate and plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "x = np.linspace(0,2,50)\n",
    "y = 1 + 2*x + stats.norm.rvs(0,1,size=len(x)) \n",
    "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "df.head()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.scatter(df[\"x\"], df[\"y\"], s=100, color=\"steelblue\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlabel(\"X\", fontsize=18)\n",
    "ax.set_ylabel(\"Y\", fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Let's start off by estimating the parameters in the regression model using statsmodels so we can check our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"x\"].values.reshape(-1,1)\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(df[\"y\"], X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The next thing we'll do is visualize the surface of the SSE, of which we're attempting to find the minimum. Does it looks like the parameters reported by sm.OLS lie at the bottom of the SSE surface?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsurface(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Now it's time to implement Stochastic Gradient Descent.  Most of the code in the function sgd has been written for you.  Your job is to fill in the values of the partial derivatives in the appropriate places.  Recall that the update scheme is given by \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 &\\leftarrow& \\beta_0 - \\eta \\cdot (-2) \\cdot \\left[y_i - (\\beta_0 + \\beta_1x_i) \\right] \\\\\n",
    "\\beta_1 &\\leftarrow& \\beta_1 - \\eta \\cdot (-2) \\cdot \\left[y_i - (\\beta_0 + \\beta_1x_i) \\right] x_i\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that the function parameter beta is a numpy array containing the initial guess for the solve. The numpy array bhist stores the approximation of the betas after each iteration for plotting purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(df, beta, eta=0.1, num_epochs=100):\n",
    "    bhist = np.zeros((num_epochs+1, 2))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    for ee in range(1,num_epochs+1):\n",
    "        shuffled_inds = list(range(len(df)))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        for ii in shuffled_inds:\n",
    "            xi = df.loc[ii,\"x\"]\n",
    "            yi = df.loc[ii,\"y\"]\n",
    "            beta[0] = # your code goes here!\n",
    "            beta[1] = # ... and here! \n",
    "        bhist[ee,:] = beta\n",
    "    return bhist \n",
    "\n",
    "beta_start = np.array([-2.0, 1.0])\n",
    "bhist = sgd(df, beta=beta_start, eta=0.0025, num_epochs=200)\n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(df, bhist=bhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Experiment with the value of the learning rate $\\eta$ and the number of epochs.  When you think you have tuned your settings, compare the parameters $\\beta_0$ and $\\beta_1$ determined by the solver with those returned by sm.OLS.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Stochastic Gradient Descent for Logistic Regression \n",
    "***\n",
    "\n",
    "In this exercise we will implement Stochastic Gradient Descent to learn the parameters of a logistic regression with a single feature.  The following code generates and plots the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1235)\n",
    "x1, x2 = stats.uniform.rvs(0,1.0,size=25), 0.8 + .5*stats.uniform.rvs(size=25)\n",
    "y1, y2 = np.zeros(25), np.ones(25)\n",
    "x, y = np.array(list(x1)+list(x2)), np.array(list(y1) + list(y2))\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "df.head()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "colors = [mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y]\n",
    "ax.scatter(df[\"x\"], df[\"y\"], s=100, color=colors)\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlabel(\"X\", fontsize=18)\n",
    "ax.set_ylabel(\"Y\", fontsize=18)\n",
    "plt.yticks([0,.25, .5, .75, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Let's start off by estimating the parameters in the logistic regression model using statsmodels so we can check our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"x\"].values.reshape(-1,1)\n",
    "X = sm.add_constant(X)\n",
    "logreg = sm.Logit(df[\"y\"], X).fit()\n",
    "logreg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The next thing we'll do is visualize the surface of the SSE, of which we're attempting to find the minimum. Does it looks like the parameters reported by sm.OLS lie at the bottom of the SSE surface?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlogregsurface(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Now it's time to implement Stochastic Gradient Descent.  Most of the code in the function sgd has been written for you.  Your job is to fill in the values of the partial derivatives in the appropriate places.  Recall that the update scheme is given by \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 &\\leftarrow& \\beta_0 - \\eta \\cdot \\left(-\\left[y_i - \\textrm{sigm}(\\beta_0 + \\beta_1x_i) \\right]\\right) \\\\\n",
    "\\beta_1 &\\leftarrow& \\beta_1 - \\eta \\cdot \\left(-\\left[y_i - \\textrm{sigm}(\\beta_0 + \\beta_1x_i) \\right] x_i\\right)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that the function parameter beta is a numpy array containing the initial guess for the solve. The numpy array bhist stores the approximation of the betas after each iteration for plotting purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_logreg(df, beta, eta=0.1, num_epochs=100):\n",
    "    bhist = np.zeros((num_epochs+1, 2))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    for ee in range(1,num_epochs+1):\n",
    "        shuffled_inds = list(range(len(df)))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        for ii in shuffled_inds:\n",
    "            xi = df.loc[ii,\"x\"]\n",
    "            yi = df.loc[ii,\"y\"]\n",
    "            mui = -(yi - sigm(beta[0] + beta[1]*xi))\n",
    "            beta[0] = # your code goes here!\n",
    "            beta[1] = # ... and here!\n",
    "        bhist[ee,:] = beta\n",
    "    return bhist \n",
    "\n",
    "beta_start = np.array([-30.0, 20.0])\n",
    "bhist = sgd_logreg(df, beta=beta_start, eta=0.1, num_epochs=7500)\n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotlogregsurface(df, bhist=bhist[::5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycolors = {\"blue\":\"steelblue\", \"red\":\"#a76c6e\",  \"green\":\"#6a9373\", \"smoke\":\"#f2f2f2\"}\n",
    "\n",
    "def eval_SSE(df, b0, b1):\n",
    "    sse = 0 \n",
    "    for ii in range(len(df)):\n",
    "        xi = df.loc[ii,\"x\"]\n",
    "        yi = df.loc[ii,\"y\"]\n",
    "        sse += (yi - (b0 + b1*xi))**2\n",
    "    return sse\n",
    "\n",
    "def plotsurface(df, bhist=None):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-1, 5, 300))\n",
    "    Z = eval_SSE(df, xx, yy)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    levels = [50, 100, 200] + list(range(400,2000,400))\n",
    "    CS = ax.contour(xx, yy, Z, levels=levels)\n",
    "    ax.clabel(CS, CS.levels, inline=True, fontsize=10)\n",
    "    ax.set_xlim([-3,3])\n",
    "    ax.set_ylim([-1,5])\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=20)\n",
    "    if bhist is not None:\n",
    "        for ii in range(bhist.shape[0]-1):\n",
    "            x0 = bhist[ii][0]\n",
    "            y0 = bhist[ii][1]\n",
    "            x1 = bhist[ii+1][0]\n",
    "            y1 = bhist[ii+1][1]\n",
    "            ax.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)\n",
    "            \n",
    "def sigm(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def eval_NLL(df, b0, b1):\n",
    "    nll = 0 \n",
    "    for ii in range(len(df)):\n",
    "        xi = df.loc[ii,\"x\"]\n",
    "        yi = df.loc[ii,\"y\"]\n",
    "        nll -= yi * np.log(sigm(b0+b1*xi)) + (1-yi)*np.log(1-sigm(b0+b1*xi))\n",
    "    return nll\n",
    "\n",
    "def plotlogregsurface(df, bhist=None):\n",
    "    xx, yy = np.meshgrid(np.linspace(-35, -10, 300), np.linspace(0, 30, 300))\n",
    "    Z = eval_NLL(df, xx, yy)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    levels = [15, 20, 50] + [150, 300, 450, 600, 750]\n",
    "    CS = ax.contour(xx, yy, Z, levels=levels)\n",
    "    ax.clabel(CS, CS.levels, inline=True, fontsize=10)\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=20)\n",
    "    if bhist is not None:\n",
    "        for ii in range(bhist.shape[0]-1):\n",
    "            x0 = bhist[ii][0]\n",
    "            y0 = bhist[ii][1]\n",
    "            x1 = bhist[ii+1][0]\n",
    "            y1 = bhist[ii+1][1]\n",
    "            ax.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
